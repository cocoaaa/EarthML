{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbon Monitoring Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import pandas as pd\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to visualize the data used in the carbon monitoring project [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) using Python tools.\n",
    "\n",
    "The goals of this notebook:\n",
    "\n",
    "* examine the measurements from each site\n",
    "* generate some visualization or global model to predict one site from every other site.\n",
    "* generate and explain model idea\n",
    "\n",
    "To run this notebook, you will need to symlink the `data` directory of the [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) to `flux_data` in the `examples` directory of `EarthML`. In addition, you will need `RSIF_2007_2016_05N_01L.mat` in the `examples` directory which you can download from https://gentinelab.eee.columbia.edu/content/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading FluxNet data ``extract_fluxnet.m``\n",
    "\n",
    "[FluxNet](http://fluxnet.fluxdata.org/) is a worldwide collection of sensor stations that record a number of local variables relating to atmospheric conditions, solar flux and soil moisture. The data is the [nee_data_fusion](https://github.com/greyNearing/nee_data_fusion/) repository is expressed as a collection of CSV files where the site names are expressed in the filenames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines functions to\n",
    "\n",
    "* read in the data from all sites\n",
    "* do some data munging (i.e., date parsing, `NaN` replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILIES_DIR = '../../nee_data_fusion/data/in_situ/fluxnet_daily/'\n",
    "METADATA_CSV = '../../nee_data_fusion/data/in_situ/extracted/allflux_metadata.txt'\n",
    "# DAILIES_DIR = 'flux_data/dailies/'\n",
    "# METADATA_CSV = 'allflux_metadata.txt'\n",
    "\n",
    "sites = [fname.split('_')[1] for fname in os.listdir(DAILIES_DIR)]\n",
    "metadata = pd.read_csv(METADATA_CSV, header=None, names=['site', 'lat', 'lon', 'igbp', 'network'], \n",
    "                       usecols=['site', 'lat', 'lon', 'igbp'], index_col='site')\n",
    "\n",
    "# get all the igbp codes for these sites\n",
    "igbp_codes = metadata.loc[sites].igbp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any missing metadata?\n",
    "metadata.loc[sites].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_days(integer):\n",
    "    \"\"\" `integer` date as `20180704` to represent July 4th, 2018\"\"\"\n",
    "    x = str(integer)\n",
    "    d = {'year': int(x[:4]), 'month': int(x[4:6]), 'day': int(x[6:])}\n",
    "    day_of_year = datetime.datetime(d['year'], d['month'], d['day']).timetuple().tm_yday\n",
    "    return day_of_year\n",
    "\n",
    "def clean(df, timestamp_col=\"TIMESTAMP\", site='', keep=[], drop=[], predict=''):\n",
    "    \"\"\"\n",
    "    Clean the dataset\n",
    "    \n",
    "    * Replace NaN's and any number less than -9990 with 0s\n",
    "    * drop columns specified in `drop`\n",
    "    * pull out prediction and feature matrices\n",
    "    * Parse timestamp and pull out day of year (\"DOY\")\n",
    "    \"\"\"\n",
    "    limit = -9990\n",
    "    for i in range(50):\n",
    "        df = df.replace(limit - i, np.nan)\n",
    "    \n",
    "    to_drop = [col for col in drop if col in df.columns]\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    for col in keep:\n",
    "        if col not in df.columns:\n",
    "            if 'SWC_F' in col or 'TS_F' in col:\n",
    "                df[col] = 0\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    df['DOY'] = df['TIMESTAMP'].apply(_parse_days)  \n",
    "    df.pop('TIMESTAMP')\n",
    "    X = df[keep]\n",
    "    y = df[predict]\n",
    "    return X, y\n",
    "\n",
    "def load_fluxnet_site(site, one=False):\n",
    "    \"\"\"\n",
    "    The main function to load data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    site : str\n",
    "        e.g., \"US-CA1\"\n",
    "    one : bool, optional\n",
    "        Whether to preform a dirty hack and create \"one\" dataframe that\n",
    "        includes the prediction variable in the feature matrix.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix. If ``one``, this will include the prediction variable\n",
    "        and be the only thing returned.\n",
    "    y : pd.DataFrame\n",
    "        The prediction variable. Not returned if ``one``.\n",
    "    \"\"\"\n",
    "    #dataRaw(dataRaw <= -9990) = 0/0 (is NaN?)\n",
    "    #NaN -> zero\n",
    "    prefix = 'FLX_{site}_FLUXNET'.format(site=site)\n",
    "    filenames = [fname for fname in os.listdir(DAILIES_DIR)\n",
    "                if fname.startswith(prefix)]\n",
    "    if len(filenames) != 1:\n",
    "        raise FileNotFoundError\n",
    "    filename = filenames[0]\n",
    "    path = '{directory}{filename}'.format(directory=DAILIES_DIR, filename=filename)\n",
    "    \n",
    "    raw_daily = pd.read_csv(path)    \n",
    "    \n",
    "    keep =  ['P_ERA',\n",
    "             'TA_ERA',\n",
    "             'PA_ERA',\n",
    "             'SW_IN_ERA',\n",
    "             'LW_IN_ERA',\n",
    "             'WS_ERA',\n",
    "             'SWC_F_MDS_1', 'SWC_F_MDS_2', 'SWC_F_MDS_3',\n",
    "             'TS_F_MDS_1', 'TS_F_MDS_2', 'TS_F_MDS_3',\n",
    "             'VPD_ERA',\n",
    "             'DOY']\n",
    "    drop = [\"GPP_DT_VUT_USTAR50\",\n",
    "            \"GPP_DT_CUT_USTAR50\",\n",
    "            \"LE_F_MDS\",\n",
    "            \"H_F_MDS\"]\n",
    "    predict = [\"NEE_CUT_USTAR50\",\n",
    "               \"NEE_VUT_USTAR50\"]\n",
    "    \n",
    "    X, y = clean(raw_daily, keep=keep, drop=drop, predict=predict[0])\n",
    "    X['site'] = site  # some metadata\n",
    "    X['y'] = y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dask\n",
    "Dask is required to read in the CSVs and do preprocessing *quickly*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(load_fluxnet_site, sites, one=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succeeded = [f for f in futures if not f.exception()]\n",
    "failed = [f for f in futures if f.exception()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = client.gather(succeeded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "\n",
    "Once the data are loaded in, they need to be joined with the metadata relating to each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)\n",
    "\n",
    "# create a little dataframe for mapping categorical variables\n",
    "a = np.zeros((len(igbp_codes), len(igbp_codes)), int)\n",
    "np.fill_diagonal(a, 1)\n",
    "categorical_igbp_mapper = pd.DataFrame(index=igbp_codes, columns=igbp_codes, data=a)\n",
    "categorical_igbp_mapper.rename_axis('igbp', inplace=True)\n",
    "\n",
    "# add metadata to the big dataframe\n",
    "df = pd.merge(df, metadata, on='site')\n",
    "df = pd.merge(df, categorical_igbp_mapper, on='igbp')\n",
    "\n",
    "print(\"df shape =\", df.values.shape)\n",
    "if float('nan') in df.columns:\n",
    "    print('nan in df.columns, removing')\n",
    "    df.pop(float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to False to not inlude vegetation type in the calculation\n",
    "include_veg = True\n",
    "\n",
    "show = df.sample(frac=0.10)\n",
    "sites = pd.Categorical(show['site']).codes\n",
    "dropped = {}\n",
    "for col in ['DOY', 'site', 'lon', 'lat', 'igbp']:\n",
    "    dropped[col] = show[col].copy()\n",
    "    show.pop(col)\n",
    "    \n",
    "if not include_veg:\n",
    "    for col in igbp_codes:\n",
    "        dropped[col] = show[col].copy()\n",
    "        show.pop(col)\n",
    "        \n",
    "print(\"{} observations and {} variables\".format(*show.shape))\n",
    "print(\"Generating a prediction with these variables: \\n  {}\".format(\n",
    "    \"\\n  \".join(list(\n",
    "        show.columns\n",
    "    ))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are sufficient to create the linear models at every site. However, the site information is hidden from the visualization algoritm.\n",
    "\n",
    "* Good sanity checks:\n",
    "    - lattitude encoded some structure, longitude does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Linear models work well *at one site* but this is confounded by\n",
    "\n",
    "* lat/lon\n",
    "* day of year\n",
    "* environment type\n",
    "\n",
    "We want to generate some visualization that accounts for these 4 variables and helps generate some understanding.\n",
    "\n",
    "That is, these observations lie on some manifold. We want to learn the structure of that manifold, and visualize each observation on that manifold.\n",
    "\n",
    "This will have the finding similar observations that have a similar structure between the indepedent variables (e.g., `P_ERA`) and depedent variables (the carbon flux measurement `y`).\n",
    "\n",
    "UMAP is a tool for this, and has firm mathematical grounding (plus, it's nice to use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reduct = umap.UMAP(verbose=True, n_epochs=None)#, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduct.fit(show.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = reduct.embedding_\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['lat', 'lon', 'igbp']\n",
    "s = pd.DataFrame(dropped)\n",
    "s['x0'] = embedding[:, 0]\n",
    "s['x1'] = embedding[:, 1]\n",
    "for col in cols:\n",
    "    if col in show:\n",
    "        s[col] = show[col]\n",
    "    else:\n",
    "        assert col in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import Select\n",
    "from bokeh.layouts import row, widgetbox\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.plotting import curdoc\n",
    "from holoviews.ipython.display_hooks import display\n",
    "import colorcet as cc\n",
    "\n",
    "colors = ['lat', 'lon', 'DOY', 'site', 'igbp']\n",
    "\n",
    "def create_figure(color='lat', **kwargs):\n",
    "    opts = {'plot': {'color_index': color, 'show_legend': False,\n",
    "                     'width': 600, 'height': 600, 'colorbar': True,\n",
    "                     'tools': ['hover']},\n",
    "            'style': {'cmap': 'magma', 'legend': False}\n",
    "}\n",
    "    if color == 'DOY':\n",
    "        opts['style']['cmap'] = cc.cm['cyclic_mrybm_35_75_c68']\n",
    "    if color == 'igbp':\n",
    "        opts['style']['cmap'] = 'Category20'\n",
    "        opts['plot']['legend_position'] ='right'\n",
    "        opts['plot']['show_legend'] = True\n",
    "    if color == 'site':\n",
    "        opts['style']['cmap'] = 'Category20'\n",
    "        opts['plot']['colorbar'] = False\n",
    "        opts['plot']['width'] = 700\n",
    "\n",
    "    opts.update(**kwargs)\n",
    "    chart = hv.Scatter(\n",
    "        s, kdims=['x0', 'x1'], vdims=[color, 'site'], extents=(-15,-15,15,15)\n",
    "    ).opts(plot=opts['plot'], style=opts['style'])\n",
    "    return display(chart)\n",
    "\n",
    "from ipywidgets import interactive\n",
    "\n",
    "w = interactive(create_figure, color=colors)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a closer look at vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igbp_vegetation = {\n",
    "    'ENF': '01 - Evergreen Needleleaf forest',\n",
    "    'EBF': '02 - Evergreen Broadleaf forest',\n",
    "    'DNF': '03 - Deciduous Needleleaf forest',\n",
    "    'DBF': '04 - Deciduous Broadleaf forest',\n",
    "    'MF': '05 - Mixed forest',\n",
    "    'CSH': '06 - Closed shrublands',\n",
    "    'OSH': '07 - Open shrublands',\n",
    "    'WSA': '08 - Woody savannas',\n",
    "    'SAV': '09 - Savannas',\n",
    "    'GRA': '10 - Grasslands',\n",
    "    'WET': '11 - Permanent wetlands',\n",
    "    'CRO': '12 - Croplands',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['vegetation'] = s['igbp'].apply(lambda x: igbp_vegetation[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = hv.Dataset(s, ['x0', 'vegetation'], ['x1', 'site'])\n",
    "grouped = ds.to(hv.Scatter, kdims=['x0', 'x1'], extents=(-15,-15,15,15), vdims=['site'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lpdaac.usgs.gov/about/news_archive/modisterra_land_cover_types_yearly_l3_global_005deg_cmg_mod12c1\n",
    "lpdaac_palette = [\n",
    "    '#008000', '#00FF00', '#99CC00', '#99FF99', '#339966', '#993366',\n",
    "    '#FFCC99', '#CCFFCC', '#FFCC00', '#FF9900', '#006699', '#FFFF00'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Scatter [width=800, height=600] (color=Cycle(lpdaac_palette), size=1, muted_alpha=0)\n",
    "grouped.overlay('vegetation').options(legend_position='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate each vegetation type and color by site id so that any site ecentricities are made clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.options(color_index='site', cmap='Category20', show_legend=False, size=1, alpha=0.8).layout().cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Linear models work well *at one site* but this is confounded by\n",
    "\n",
    "* lat/lon\n",
    "* day of year\n",
    "* environment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "assert 'site' not in show.columns\n",
    "y = show['y'].values\n",
    "X = pd.DataFrame({col: show[col].values \n",
    "                  for col in show.columns \n",
    "                  if col != 'y'})\n",
    "print(X.shape)\n",
    "assert 'y' not in X.columns\n",
    "\n",
    "# tranform data matrix so 0 mean, unit variance for each feature\n",
    "X = StandardScaler().fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def predict(X_train, X_test, y_train):\n",
    "    # find the nearest neighbors\n",
    "    neighbors = NearestNeighbors(n_neighbors=50)\n",
    "    neighbors.fit(X_train)\n",
    "    idx = neighbors.kneighbors(X_test, return_distance=False)\n",
    "\n",
    "    # train and fit a linear model of those values\n",
    "    y_hat = []\n",
    "    for point, neighbors in zip(X_test, idx):\n",
    "        _X_train = X_train[neighbors]\n",
    "        _y_train = y_train[neighbors]\n",
    "    \n",
    "        model = LinearRegression()\n",
    "        model.fit(_X_train, _y_train)\n",
    "        y_hat += [model.predict(point.reshape(1, -1))]\n",
    "    return np.array(y_hat).flat[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **TODO:** Integrate with Dask (likely wrwapping predict calls with `dask.delayed`)\n",
    "* **TODO:** Select better method of finding neighbors\n",
    "    * smart way of figuring out neighbors? Probably all observations within a certain radius\n",
    "    * use the embedding (only if radius method doesn't work, requires reading UMAP paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "sep = LeaveOneGroupOut()\n",
    "corrs = []\n",
    "n_splits = sep.get_n_splits(X, y, sites)\n",
    "for i, (train_index, test_index) in enumerate(sep.split(X, y, sites)):\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        print('{:0.0f}% complete'.format(100 * i / n_splits))\n",
    "    y_hat = predict(X[train_index],\n",
    "                    X[test_index],\n",
    "                    y[train_index])\n",
    "    corrs += [np.corrcoef(y_hat, y[test_index])[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = np.array(corrs)\n",
    "frequencies, edges = np.histogram(corrs, 20)\n",
    "\n",
    "c1 = hv.Histogram((frequencies, edges))\n",
    "c2 = hv.VLine(np.mean(corrs), label='mode')\n",
    "c1 * c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of modifications\n",
    "`----------------------------------------------------`\n",
    "\n",
    "\n",
    "The rest of this notebook is copy-pasted from `Carbon-FLux.ipynb`. I haven't modified it yet.\n",
    "\n",
    "`----------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding RSIF data ``collocate_data_types.m``\n",
    "\n",
    "RSIF is the 'Reconstructed Solar Induced Fluorescence' expressing solar energy flux (power) per meter squared arriving on the Earth's surface derived from a vegetation signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "rsif = scipy.io.loadmat('RSIF_2007_2016_05N_01L.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The goal now is to add the RSIF time series added at positions of stations, then normalize the time dimension (resample to make the data sets have the same temporal sampling) and finally to perform a linear regression analysis on the combined data.\n",
    "\n",
    "Note that the matlab file reference above was downloaded from https://gentinelab.eee.columbia.edu/content/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image [width=700 height=500 clipping_colors={'NaN': 'gray'}] Points (marker='x' color='cyan')\n",
    "\n",
    "mdata = rsif['RSIF'] # NaNs outside of land area.\n",
    "def rsif_image(day):\n",
    "    return hv.Image(mdata[:,:,day], kdims=['lon', 'lat'], vdims=['RSIF'], bounds=(-180,-90,180,90))\n",
    "\n",
    "rsif_dmap = hv.DynamicMap(rsif_image, kdims=['day']).redim.values(day=range(mdata.shape[2]))\n",
    "raw_site_positions = {site:site_lat_lon(metadata, site) for site in sites}\n",
    "site_positions = {site:(lon,lat) for (site, (lat,lon)) in raw_site_positions.items() if None not in (lat,lon)}\n",
    "rsif_dmap * hv.Points(site_positions.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithm might be useful: Smoothing?\n",
    "* Mismatched temporospatial satellite imagery.\n",
    "\n",
    "1. Timestamp per lat/lon. Comes with triples. \n",
    "2. Polar: e.g 1pm local time.\n",
    "3. Global (processed product?)\n",
    "\n",
    "https://science.nasa.gov/earth-science/earth-science-data/data-processing-levels-for-eosdis-data-products\n",
    "\n",
    "1. Level 0: Raw data (direct sensor flux). Highest resolution. Minimal model - good for ML.\n",
    "2. Level 1: Sensor geometry.\n",
    "3. Level 2: spatial regridding  \n",
    "4. Level 3: check: physical variable. Primary science product.\n",
    "5. Level 4: check: model added value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the RSIF signal at the sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tables = []\n",
    "for day in range(mdata.shape[2]):\n",
    "    image = rsif_dmap.select(day=day)\n",
    "    table = image.sample(samples=site_positions.values())\n",
    "    tables.append(table.add_dimension('site', 0, site_positions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabulated = hv.HoloMap({day:tables[day] for day in range(mdata.shape[2])}, kdims=['Day'])\n",
    "tabulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the RSIF time series per site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_RSIF = tabulated.table().to(hv.Curve, 'Day', 'RSIF').drop_dimension(['lat', 'lon'])\n",
    "site_RSIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biweekly_averaging.m\n",
    "\n",
    "'RSIF' is put onto the same time sampling as the fluxnet data, adding an 'RSIF' field to the fluxnet dataframe for each site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main_lr_fluxnet.m\n",
    "\n",
    "\n",
    "Finding the informative variables.\n",
    "\n",
    "Main step:\n",
    "\n",
    "```\n",
    "model{s} = stepwiselm(X,y,''constant'',''Criterion'',''aic'',''Upper'',''linear'')\n",
    "```\n",
    "\n",
    "Using [stepwiselm](https://uk.mathworks.com/help/stats/stepwiselm.html):\n",
    "\n",
    "```\n",
    "mdl = stepwiselm(X,y,modelspec) creates a linear model of the responses y to the predictor variables in the data matrix X, using stepwise regression to add or remove predictors. modelspec is the starting model for the stepwise procedure.\n",
    "```\n",
    "\n",
    "Where ``allData`` is the table (effectively the dataframe):\n",
    "\n",
    "```\n",
    "cols= [3:11,14:22];\n",
    "X = allData(:,cols,s);\n",
    "y = allData(:,12,s);\n",
    "```\n",
    "\n",
    "* 3:11 : 'LAT','LON','P_ERA','TA_ERA','PA_ERA','SW_IN_ERA','LW_IN_ERA','WS_ERA','LE_F_MDS'\n",
    "* 12: 'H_F_MDS' (Sensible heat flux, gapfilled using MDS method)\n",
    "\n",
    "Check. NEE is the desired predicted. Column 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main_ann_fluxnet.m\n",
    "\n",
    "Main step for size ``s`` in ``trainANN`` using [``train``](https://uk.mathworks.com/help/nnet/ref/train.html) in the neural network toolbox:\n",
    "\n",
    "```\n",
    "% net = fitnet(parms.nodes,parms.trainFcn);\n",
    "net = feedforwardnet(parms.nodes,parms.trainFcn);\n",
    "[net,~] = train(net,x,t);\n",
    "```\n",
    "\n",
    "```\n",
    "Xdex= [2:10,15,18,21,24];\n",
    "Ydex = 13;\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
